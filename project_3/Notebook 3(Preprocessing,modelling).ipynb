{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2dfe506a-7a32-4460-a221-c1a42fa01c5d",
   "metadata": {},
   "source": [
    "# Notebook 3 (Preprocessing and modelling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6b6eae-bc95-45bd-ba6d-7efc48a9088d",
   "metadata": {},
   "source": [
    "### Content\n",
    "- Introduction\n",
    "- Preprocessing\n",
    "- Visualization after preprocessing is completed\n",
    "- Modelling\n",
    "- Evaluation of models\n",
    "- Future work and recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfdc066-d752-4714-b843-ba6233b12521",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a44380c-893d-49c1-a11d-db98cf889697",
   "metadata": {},
   "source": [
    "In this notebook, we will perform the preprocessing step to prepare the data for modelling. We aim to create a model that will be able to classify if a post belongs to the subreddit of Starbucks or DunkinDonuts. We will then evaluate the models with metrics such as accuracy score, recall score, precision score and F1 score\n",
    "\n",
    "- Vectorizer: TF-IDF Vectorizer, Count Vectorizer\n",
    "\n",
    "#### Models:\n",
    "- Logistic Regression\n",
    "- Random Forest Classification\n",
    "- SVM \n",
    "- Ridge classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e06f4d2-4565-4cd2-988a-9cfc9ed1ac96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "    \n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer # RegexpTokenizer uses regex for tokenization\n",
    "from nltk.stem import WordNetLemmatizer # for lemmatization\n",
    "from nltk.stem.porter import PorterStemmer # for stemming\n",
    "from nltk.corpus import stopwords # for stopwords removal\n",
    "import nltk\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.pipeline import Pipeline # to compactly pack multiple modeling operations\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix,ConfusionMatrixDisplay, accuracy_score, recall_score, precision_score, f1_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier, VotingClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "\n",
    "# Import CountVectorizer and TFIDFVectorizer from feature_extraction.text.\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction import text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676ee34b-f723-4855-aa70-1657b7b569de",
   "metadata": {},
   "source": [
    "### Import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6cc238-79ae-4b3c-b628-8e85ffb6463d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/combined_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c606966-5b02-41a6-9eb2-50efafc1119c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9973, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>words_in_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>My new favorite 50 stars drink! Caffe misto wi...</td>\n",
       "      <td>starbucks</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>How does your store handle partners buying mer...</td>\n",
       "      <td>starbucks</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>8am.. ü§¶üèª‚Äç‚ôÄÔ∏è</td>\n",
       "      <td>starbucks</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Looking for the gay male baristas?</td>\n",
       "      <td>starbucks</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Reasons why I hate my job.</td>\n",
       "      <td>starbucks</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              title  subreddit  \\\n",
       "0           0  My new favorite 50 stars drink! Caffe misto wi...  starbucks   \n",
       "1           1  How does your store handle partners buying mer...  starbucks   \n",
       "2           2                                        8am.. ü§¶üèª‚Äç‚ôÄÔ∏è  starbucks   \n",
       "3           3                 Looking for the gay male baristas?  starbucks   \n",
       "4           4                         Reasons why I hate my job.  starbucks   \n",
       "\n",
       "   words_in_sentence  \n",
       "0                 23  \n",
       "1                  8  \n",
       "2                  1  \n",
       "3                  6  \n",
       "4                  6  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bd808d-f14e-4445-b2a0-129f9a6043c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>words_in_sentence</th>\n",
       "      <th>starbucks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My new favorite 50 stars drink! Caffe misto wi...</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How does your store handle partners buying mer...</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8am.. ü§¶üèª‚Äç‚ôÄÔ∏è</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Looking for the gay male baristas?</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Reasons why I hate my job.</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  words_in_sentence  \\\n",
       "0  My new favorite 50 stars drink! Caffe misto wi...                 23   \n",
       "1  How does your store handle partners buying mer...                  8   \n",
       "2                                        8am.. ü§¶üèª‚Äç‚ôÄÔ∏è                  1   \n",
       "3                 Looking for the gay male baristas?                  6   \n",
       "4                         Reasons why I hate my job.                  6   \n",
       "\n",
       "   starbucks  \n",
       "0          1  \n",
       "1          1  \n",
       "2          1  \n",
       "3          1  \n",
       "4          1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['starbucks']=df['subreddit'].map({'starbucks':1,'DunkinDonuts':0})\n",
    "df.drop(columns=['subreddit','Unnamed: 0'] , axis = 1, inplace = True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41831046-db77-4790-9ba8-9bafe5d0bc54",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190c123b-61c7-47fa-b81b-a4b7222afad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting the text in 'title' to lowercase\n",
    "df['title_cleaned'] = df['title'].apply(lambda x: \" \".join(x.lower()\n",
    "for x in x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee9562e-c0dd-4880-bff5-9c541124d783",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove numbers\n",
    "df['title_cleaned'].replace('\\d+', '', regex=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75eb7717-be07-4b11-9754-b7f3690228be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rq/jvgm9dz559g9xyvpqqc2rlcm0000gn/T/ipykernel_46683/2256789894.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['title_cleaned'].str.replace('http\\S+','')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0       my new favorite  stars drink! caffe misto with...\n",
       "1       how does your store handle partners buying mer...\n",
       "2                                              am.. ü§¶üèª‚Äç‚ôÄÔ∏è\n",
       "3                      looking for the gay male baristas?\n",
       "4                              reasons why i hate my job.\n",
       "                              ...                        \n",
       "9968    dunkin‚Äô reddit! please take a moment to fill o...\n",
       "9969                            discontinuing strawberry?\n",
       "9970                                      are these good?\n",
       "9971                                     peach refresher?\n",
       "9972    i‚Äôve been going to dunkin forever and i really...\n",
       "Name: title_cleaned, Length: 9973, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Remove URL\n",
    "df['title_cleaned'].str.replace('http\\S+','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5937d4e6-8246-4f1e-8a3e-64b72cd0dce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rq/jvgm9dz559g9xyvpqqc2rlcm0000gn/T/ipykernel_46683/628797242.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['title_cleaned'] = df['title_cleaned'].str.replace(r'[^\\w\\s]+', '')\n"
     ]
    }
   ],
   "source": [
    "#Remove punctuation\n",
    "df['title_cleaned'] = df['title_cleaned'].str.replace(r'[^\\w\\s]+', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52dc0a5d-5cf4-4aa6-84c3-41accfc99f30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f3b0b6-06bd-40f0-822c-d80940e509fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stem' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m lemmatizer \u001b[38;5;241m=\u001b[39m WordNetLemmatizer()\n\u001b[1;32m      3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m RegexpTokenizer(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlem_words\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtitle_cleaned\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mstem\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mword\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlem_title\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlem_words\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(x))\n",
      "File \u001b[0;32m~/mambaforge/envs/dsi-sg/lib/python3.8/site-packages/pandas/core/series.py:4433\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4323\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4324\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4325\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4328\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4329\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4330\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4331\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4332\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4431\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4432\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/dsi-sg/lib/python3.8/site-packages/pandas/core/apply.py:1088\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1084\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   1085\u001b[0m     \u001b[38;5;66;03m# if we are a string, try to dispatch\u001b[39;00m\n\u001b[1;32m   1086\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[0;32m-> 1088\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/dsi-sg/lib/python3.8/site-packages/pandas/core/apply.py:1143\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1137\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m   1138\u001b[0m         \u001b[38;5;66;03m# error: Argument 2 to \"map_infer\" has incompatible type\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m         \u001b[38;5;66;03m# \"Union[Callable[..., Any], str, List[Union[Callable[..., Any], str]],\u001b[39;00m\n\u001b[1;32m   1140\u001b[0m         \u001b[38;5;66;03m# Dict[Hashable, Union[Union[Callable[..., Any], str],\u001b[39;00m\n\u001b[1;32m   1141\u001b[0m         \u001b[38;5;66;03m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[39;00m\n\u001b[1;32m   1142\u001b[0m         \u001b[38;5;66;03m# \"Callable[[Any], Any]\"\u001b[39;00m\n\u001b[0;32m-> 1143\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1146\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1150\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1151\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/mambaforge/envs/dsi-sg/lib/python3.8/site-packages/pandas/_libs/lib.pyx:2870\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      2\u001b[0m lemmatizer \u001b[38;5;241m=\u001b[39m WordNetLemmatizer()\n\u001b[1;32m      3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m RegexpTokenizer(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlem_words\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle_cleaned\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: tokenizer\u001b[38;5;241m.\u001b[39mtokenize(x))\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: [stem\u001b[38;5;241m.\u001b[39mstem(word) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m x])\n\u001b[1;32m      6\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlem_title\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlem_words\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(x))\n",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m lemmatizer \u001b[38;5;241m=\u001b[39m WordNetLemmatizer()\n\u001b[1;32m      3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m RegexpTokenizer(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlem_words\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle_cleaned\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: tokenizer\u001b[38;5;241m.\u001b[39mtokenize(x))\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: [\u001b[43mstem\u001b[49m\u001b[38;5;241m.\u001b[39mstem(word) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m x])\n\u001b[1;32m      6\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlem_title\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlem_words\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(x))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stem' is not defined"
     ]
    }
   ],
   "source": [
    "#Lemmatized text in 'title_cleaned'\n",
    "stem = WordNetLemmatizer()\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "df['stem_words'] = df['title_cleaned'].apply(lambda x: tokenizer.tokenize(x)).apply(lambda x: [lemmatizer.stem(word) for word in x])\n",
    "df['stem_title'] = df['stem_words'].apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe9ac1f-613f-4cc6-81e8-2ce589470658",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8af839-788d-4680-8c2d-b5bc856b7e02",
   "metadata": {},
   "source": [
    "#### Visualization for common words after stemming text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94840dc6-63f0-4668-9d80-5fd0ccad508c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec = CountVectorizer(stop_words='english', ngram_range = (1,1))\n",
    "\n",
    "sb_cvec = cvec.fit_transform(df[df['starbucks']==1]['stem_title'])\n",
    "sb_cvec = pd.DataFrame(sb_cvec.toarray(),\n",
    "                       columns = cvec.get_feature_names_out())\n",
    "sb_top_words = sb_cvec.sum().sort_values(ascending=False)\n",
    "sb_top_60 = sb_top_words.head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713c8528-5800-48c5-8648-b9431a7c5291",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_cvec = cvec.fit_transform(df[df['starbucks']==0]['stem_title'])\n",
    "dd_cvec = pd.DataFrame(dd_cvec.toarray(),\n",
    "                       columns = cvec.get_feature_names_out())\n",
    "dd_top_words = dd_cvec.sum().sort_values(ascending=False)\n",
    "dd_top_60 = dd_top_words.head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f61080-6c4e-4b84-a3ea-c77ea1c76559",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows = 1, ncols = 2, figsize = (16,12), sharex = True)\n",
    "                      \n",
    "sns.barplot( x=sb_top_60, y=sb_top_60.index.values,ax=ax[0])\n",
    "ax[0].set_title('Top 60 words in starbucks dataframe')\n",
    "ax[0].set_xlabel('Word Count')               \n",
    "sns.barplot( x=dd_top_60, y=dd_top_60.index.values,ax=ax[1])\n",
    "ax[1].set_title('Top 60 words in dunkindonuts dataframe')\n",
    "ax[1].set_xlabel('Word Count')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d031495d-714f-40e4-be48-216bef11e7cf",
   "metadata": {},
   "source": [
    "We will remove additional stopwords which will not help us in identifying if a post belongs to starbucks or dunkin donuts subreddit\n",
    "These sort of post includes\n",
    "- Posts that contain the brand names ie. starbucks, dunkin\n",
    "- other words that contain no meaning on it's own ie. 'wa', 'got', 'im'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af2c422-b4f7-44ef-b61c-6385f0f7ad13",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = ['starbuck','thi','anyon','just','make','im','wa','doe','know','whi','got','ha','come','onli','say','dunkin','donut','get']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b30de07-4094-407c-8791-dc9e47a54be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_stopwords = stopwords.words('english')\n",
    "final_stopwords+=stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0e8eba-3243-4c49-ac41-37ae712d5c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec = CountVectorizer(stop_words=final_stopwords, ngram_range = (1,1))\n",
    "\n",
    "sb_cvec = cvec.fit_transform(df[df['starbucks']==1]['stem_title'])\n",
    "sb_cvec = pd.DataFrame(sb_cvec.toarray(),\n",
    "                       columns = cvec.get_feature_names_out())\n",
    "sb_top_words = sb_cvec.sum().sort_values(ascending=False)\n",
    "sb_top_60 = sb_top_words.head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3b17b8-e818-4d97-af7b-61c5e105ce1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_cvec = cvec.fit_transform(df[df['starbucks']==0]['stem_title'])\n",
    "dd_cvec = pd.DataFrame(dd_cvec.toarray(),\n",
    "                       columns = cvec.get_feature_names_out())\n",
    "dd_top_words = dd_cvec.sum().sort_values(ascending=False)\n",
    "dd_top_60 = dd_top_words.head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd0b0fb-fea4-40f6-adc3-395161d57ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows = 1, ncols = 2, figsize = (16,12), sharex = True)\n",
    "                      \n",
    "sns.barplot( x=sb_top_60, y=sb_top_60.index.values,ax=ax[0])\n",
    "ax[0].set_title('Top 60 words in starbucks dataframe(revised after stopwords removed)')\n",
    "ax[0].set_xlabel('Word Count')               \n",
    "sns.barplot( x=dd_top_60, y=dd_top_60.index.values,ax=ax[1])\n",
    "ax[1].set_title('Top 60 words in dunkindonuts dataframe(revised after stopwords removed)')\n",
    "ax[1].set_xlabel('Word Count')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddd3059-c2de-4308-b129-724d7aa597b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "#Quick check with WordCloud for most prominent words\n",
    "wc_sb = WordCloud(background_color=\"white\", colormap=\"Dark2\",width=1000, height=800,\n",
    "               max_words=50, random_state=42).generate_from_frequencies(sb_top_60)\n",
    "\n",
    "plt.figure(figsize = (15,10) , facecolor = 'k')\n",
    "plt.imshow(wc_sb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02078459-08c8-4a35-84e1-15234e82a20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wc_dd = WordCloud(background_color=\"white\", colormap=\"winter\",width=1000, height=800,\n",
    "               max_words=50, random_state=42).generate_from_frequencies(dd_top_60)\n",
    "\n",
    "plt.figure(figsize = (15,10) , facecolor = 'k')\n",
    "plt.imshow(wc_dd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ccc6f9-8506-4a78-a9aa-216e33c56f0a",
   "metadata": {},
   "source": [
    "#### Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1014b76b-06e4-419e-b234-c5e5fdc0a55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec = CountVectorizer(stop_words=final_stopwords, ngram_range = (2,2))\n",
    "\n",
    "sb_cvec = cvec.fit_transform(df[df['starbucks']==1]['stem_title'])\n",
    "sb_cvec = pd.DataFrame(sb_cvec.toarray(),\n",
    "                       columns = cvec.get_feature_names_out())\n",
    "sb_top_words = sb_cvec.sum().sort_values(ascending=False)\n",
    "sb_top_60 = sb_top_words.head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29537d0-342a-4390-87c7-e5bc1c96a4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_cvec = cvec.fit_transform(df[df['starbucks']==0]['stem_title'])\n",
    "dd_cvec = pd.DataFrame(dd_cvec.toarray(),\n",
    "                       columns = cvec.get_feature_names_out())\n",
    "dd_top_words = dd_cvec.sum().sort_values(ascending=False)\n",
    "dd_top_60 = dd_top_words.head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e30749c-6ddd-4c09-838e-670b77b3d4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows = 1, ncols = 2, figsize = (22,12), sharex = True)\n",
    "                      \n",
    "sns.barplot( x=sb_top_60, y=sb_top_60.index.values,ax=ax[0])\n",
    "ax[0].set_title('Top 60 bigram words in starbucks dataframe(revised after stopwords removed)')\n",
    "ax[0].set_xlabel('Word Count')               \n",
    "sns.barplot( x=dd_top_60, y=dd_top_60.index.values,ax=ax[1])\n",
    "ax[1].set_title('Top 60 bigram words in dunkindonuts dataframe(revised after stopwords removed)')\n",
    "ax[1].set_xlabel('Word Count')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c94ccd9-753f-4c25-b372-73344870b543",
   "metadata": {},
   "source": [
    "Observations\n",
    "- The words 'iced coffee' and 'cold brew' will likely classify the post to dunkin donuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57bc5f6-a69e-441f-a912-18f858797e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "df['tokenized'] = df['stem_title'].apply(lambda x: tokenizer.tokenize(x))\n",
    "df['words_in_title'] = df['tokenized'].apply(len)\n",
    "df[df['words_in_title'] < 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdbe1fb-3529-488f-a705-1cd535a4ce2d",
   "metadata": {},
   "source": [
    "We will remove all rows that are left with 1 word in title after stemming, this is because these post with 1 word might skew the performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ee26f1-6919-4ac7-a3be-af9d7f807ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['words_in_title']>=2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bbbdaa-0c27-4e15-a62e-0c082d01f1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b404957-bac6-4cb8-9f3f-0bd6d4a41006",
   "metadata": {},
   "source": [
    "Removed 306 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517ba3f0-7ec0-4b50-9c16-98b0530afa64",
   "metadata": {},
   "source": [
    "#### Saving dataframe for modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e5592f-8217-4775-8810-4be454696e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/df_model.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9089daf-b733-4d04-9412-621ad315dff9",
   "metadata": {},
   "source": [
    "## Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc1cd4a-eae5-4860-bc91-4ad5ada8b17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['stem_title']\n",
    "y = df['starbucks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb98d233-3552-4004-9763-9dd9f71ec324",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y, \n",
    "                                                    test_size=0.30, \n",
    "                                                    random_state=42, \n",
    "                                                    stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763fb1eb-581f-4474-83b9-136f9b1e6caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['starbucks'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fd5797-1a8f-48f0-a764-187ea78bd127",
   "metadata": {},
   "source": [
    "### Baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed29ae0-abed-4540-a8fe-9c05c45a44e4",
   "metadata": {},
   "source": [
    "Our basline model will be 0.5. There is only a 50% chance that this model will predict a text to be from starbucks subreddit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff82e477-7638-42f4-8109-95d3a555a413",
   "metadata": {},
   "source": [
    "#### TF-IDF and Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a955f5-842d-4fec-8f02-38215ab0b723",
   "metadata": {},
   "source": [
    "I choose to use TF-IDF vectorizer ahead of Count Vectorizer because TF-IDF is a score that tells us which words are important to one document, relative to all other documents. Words that occur often in one document but don't occur in many documents contain more predictive power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a8dcc7-655c-4910-813a-fe1657934bc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tvec = TfidfVectorizer(stop_words=final_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc381e0-3bc1-47a8-a38e-a0c9164e382c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train_tvec = pd.DataFrame(tvec.fit_transform(X_train).todense(),\n",
    "                       columns=tvec.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32be289a-555a-4f48-a955-13fa05e966eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tvec = pd.DataFrame(tvec.transform(X_test).todense(),\n",
    "                       columns=tvec.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc05a5b-7565-45fc-a25f-7f6bfe6b7050",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd785157-161c-4608-b784-e1ef32ce8ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(X_train_tvec,y_train)\n",
    "\n",
    "pred=lr.predict(X_test_tvec)\n",
    "\n",
    "train_score = lr.score(X_train_tvec,y_train)\n",
    "test_score = lr.score(X_test_tvec,y_test)\n",
    "cv_score=cross_val_score(lr,X_train_tvec,y_train,cv=10).mean()\n",
    "\n",
    "print(f'Training Score(Logreg) : {train_score}')\n",
    "print(f'Testing Score(Logreg) : {test_score}')\n",
    "print(f'Cross Validation Score(Logreg) : {cv_score}')\n",
    "print('\\n')\n",
    "\n",
    "accuracy_lr = accuracy_score(y_test, pred)\n",
    "recall_lr = recall_score(y_test,pred)\n",
    "precision_lr = precision_score(y_test,pred)\n",
    "f1_score_lr = f1_score(y_test,pred)\n",
    "\n",
    "print(f'Accuracy score : {accuracy_lr}')\n",
    "print(f'Recall score : {recall_lr}')\n",
    "print(f'Precision score : {precision_lr}')\n",
    "print(f'f1 score : {f1_score_lr}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68112bd-7343-45f1-960d-b67dc44de345",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exploring the Coefs that are strong indicators for model performance\n",
    "coef_table = pd.DataFrame(list(X_train_tvec.columns)).copy()\n",
    "coef_table.insert(len(coef_table.columns),\"Coefs\",lr.coef_.transpose())\n",
    "coef_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246318e3-fe1d-4866-aef7-64a334a8cdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "highest_30_coef = coef_table.sort_values(by=\"Coefs\",ascending = False).head(30)\n",
    "lowest_30_coef = coef_table.sort_values(by=\"Coefs\",ascending = False).tail(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e25e555-e3e4-4330-8e01-c92c497f2df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top 30 words for predicting Starbucks\")\n",
    "highest_30_coef"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f28c86e-0725-4b25-9125-362caa851511",
   "metadata": {},
   "source": [
    "Including additional stopwords to improve model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68be3013-7c88-4305-868c-4983e3811362",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_stopwords = final_stopwords + ['star','sm','venti','grande','ssv','tall','asm','psl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5ad2b9-f7c0-4da6-a298-98bd0d1378f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top 30 words for predicting DunkinDonuts\")\n",
    "lowest_30_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4762285f-d315-4547-a20b-da203ebb49c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_stopwords = final_stopwords + ['tmobil','dunk','larg','dd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2122df60-3d5e-4842-aaf8-fa90a1f8c7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for improvement of scores after removing stop words\n",
    "tvec = TfidfVectorizer(stop_words=final_stopwords)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y, \n",
    "                                                    test_size=0.30, \n",
    "                                                    random_state=42, \n",
    "                                                    stratify=y)\n",
    "\n",
    "X_train_tvec = pd.DataFrame(tvec.fit_transform(X_train).todense(),\n",
    "                       columns=tvec.get_feature_names_out())\n",
    "X_test_tvec = pd.DataFrame(tvec.transform(X_test).todense(),\n",
    "                       columns=tvec.get_feature_names_out())\n",
    "\n",
    "lr.fit(X_train_tvec,y_train)\n",
    "\n",
    "pred = lr.predict(X_test_tvec)\n",
    "\n",
    "train_score = lr.score(X_train_tvec,y_train)\n",
    "test_score = lr.score(X_test_tvec,y_test)\n",
    "cv_score=cross_val_score(lr,X_train_tvec,y_train,cv=10).mean()\n",
    "\n",
    "print(f'Training Score(Logreg) : {train_score}')\n",
    "print(f'Testing Score(Logreg) : {test_score}')\n",
    "print(f'Cross Validation Score(Logreg) : {cv_score}')\n",
    "print('\\n')\n",
    "\n",
    "accuracy_lr = accuracy_score(y_test, pred)\n",
    "recall_lr = recall_score(y_test,pred)\n",
    "precision_lr = precision_score(y_test,pred)\n",
    "f1_score_lr = f1_score(y_test,pred)\n",
    "\n",
    "print(f'Accuracy score : {accuracy_lr}')\n",
    "print(f'Recall score : {recall_lr}')\n",
    "print(f'Precision score : {precision_lr}')\n",
    "print(f'f1 score : {f1_score_lr}')\n",
    "\n",
    "#Confusion Matrix Visualization\n",
    "cm_lr = confusion_matrix(y_test, pred)  \n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm_lr, display_labels=['starbucks', 'dunkindonuts'])\n",
    "disp.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b568f4-1dce-470f-8871-c7c4cc07612a",
   "metadata": {},
   "source": [
    "*No Significant improvements in score after removing stopwords*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdaaf72-895c-44d7-85f8-eadf229b4b46",
   "metadata": {},
   "source": [
    "#### TF-IDF and Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fab0e3-90cf-4f00-9958-16cec47859c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tvec = TfidfVectorizer(stop_words=final_stopwords)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y, \n",
    "                                                    test_size=0.30, \n",
    "                                                    random_state=42, \n",
    "                                                    stratify=y)\n",
    "\n",
    "X_train_tvec = pd.DataFrame(tvec.fit_transform(X_train).todense(),\n",
    "                       columns=tvec.get_feature_names_out())\n",
    "X_test_tvec = pd.DataFrame(tvec.transform(X_test).todense(),\n",
    "                       columns=tvec.get_feature_names_out())\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "rf.fit(X_train_tvec,y_train)\n",
    "\n",
    "pred = rf.predict(X_test_tvec)\n",
    "\n",
    "train_score = rf.score(X_train_tvec,y_train)\n",
    "test_score = rf.score(X_test_tvec,y_test)\n",
    "cv_score = cross_val_score(rf,X_train_tvec,y_train,cv=10).mean()\n",
    "\n",
    "print(f'Training Score(Random Forest) : {train_score}')\n",
    "print(f'Testing Score(Random Forest) : {test_score}')\n",
    "print(f'Cross Validation Score(Random Forest) : {cv_score}')\n",
    "print('\\n')\n",
    "\n",
    "accuracy_rf = accuracy_score(y_test, pred)\n",
    "recall_rf = recall_score(y_test,pred)\n",
    "precision_rf = precision_score(y_test,pred)\n",
    "f1_score_rf = f1_score(y_test,pred)\n",
    "\n",
    "print(f'Accuracy score : {accuracy_rf}')\n",
    "print(f'Recall score : {recall_rf}')\n",
    "print(f'Precision score : {precision_rf}')\n",
    "print(f'f1 score : {f1_score_rf}') \n",
    "\n",
    "#Confusion Matrix Visualization\n",
    "cm_rf = confusion_matrix(y_test, pred)  \n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm_rf, display_labels=['starbucks', 'dunkindonuts'])\n",
    "disp.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e1b2f5-ce20-45fe-a958-eb8f306c2404",
   "metadata": {},
   "source": [
    "Train set accuracy score is alot better than the test score, suggesting that this model might be overfitted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7dcc1b-bb4d-4290-bbcc-dfda8351e565",
   "metadata": {},
   "source": [
    "|           | Baseline | Logistic Regression | Random Forest |\n",
    "|-----------|----------|---------------------|---------------|\n",
    "| Accuracy  | 0.5      | 0.725               | 0.699       |\n",
    "| Precision |          | 0.718               | 0.706         |\n",
    "| Recall    |          | 0.74                | 0.684         |\n",
    "| F1        |          | 0.73                | 0.695        |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1f7f61-a786-487c-b4ab-5a2bda66b883",
   "metadata": {},
   "source": [
    "Logistic regression and random forest are performed here in order to serve as our new baseline model after beating our null model. We will then use the low code ML libray Pycaret to further explore other models which could potentially be an improvements on these 2 models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec74306-a101-44d5-9201-28fd7f8fbae9",
   "metadata": {},
   "source": [
    "### Modelling with Pycaret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f72498-4236-4008-8b31-c73f18431829",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyca_df = pd.concat([X_train_tvec, y_train.reset_index(drop=True)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6924707e-86f2-44d6-8b2a-091994a92478",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyca_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34df0f52-531d-44c5-b086-4b59892ba190",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = list(pyca_df.columns)\n",
    "numeric_cols = numeric_cols[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ddce93-7b9d-4174-8496-5202dee94e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycaret.classification import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fb635f-fa97-4bfe-b881-2cdd2aa74517",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = setup(data = pyca_df, numeric_features=numeric_cols, target = 'starbucks', session_id=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3091da-4b8c-463c-81c1-9ca6387e8926",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = compare_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db79e60-5e65-43d3-a302-bc14af87b61e",
   "metadata": {},
   "source": [
    "Model to be implemented:\n",
    "- Logistic Regression\n",
    "- SVM\n",
    "- Ridge Classifier\n",
    "- Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001691ff-9c11-4f05-8f58-070633e3f1e3",
   "metadata": {},
   "source": [
    "#### Logistic Regression with Pycaret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856bf89c-281a-43ce-95a7-466b46c1ea1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = create_model('lr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1f195a-daf3-4dd0-8544-163a1307e06f",
   "metadata": {},
   "source": [
    "#### Tune model (Logistic Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed13265-bba5-455b-92a6-f907ae6346f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_lr = tune_model(lr_model, choose_better=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee3cb59-46d0-4209-90be-72a5be1979cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(tuned_lr, plot = 'confusion_matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb2f6dd-9193-4175-a7e1-09f6ed11bc40",
   "metadata": {},
   "source": [
    "#### SVM with Pycaret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6823936-d919-4922-89b4-fda7bdbd90a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model = create_model('svm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174b9427-11e0-489b-b1a8-47394716fa48",
   "metadata": {},
   "source": [
    "#### Tune model (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f67713a-6d5a-45c8-a23c-18e8a5ca50cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_svm = tune_model(svm_model, choose_better=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bac4cd-3148-403d-8090-be7d698c9221",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(svm_model, plot = 'confusion_matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ada5c76-32ae-4bfb-b304-0786b1ec9400",
   "metadata": {},
   "source": [
    "#### Ridge Classification with Pycaret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f94bb7-d809-458b-9c25-24133c539a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_model = create_model('ridge')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d6a485-74aa-4858-8ef4-acdb45e3ccc3",
   "metadata": {},
   "source": [
    "#### Tune model (Ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad957f2c-0f57-4d87-912a-bc5229afa802",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_ridge = tune_model(ridge_model, choose_better=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2480c133-2152-4ede-8eef-51ce4f340f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(tuned_ridge, plot = 'confusion_matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e136fd3e-65cf-40b5-8e08-5dd3f2b42704",
   "metadata": {},
   "source": [
    "#### Random Forest with Pycaret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c550cd-c249-4061-9ef8-6735e17b17bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = create_model('rf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54555f03-51a4-4433-801f-0b97acd27990",
   "metadata": {},
   "source": [
    "#### Tune model (Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09e8eeb-cf23-4d1b-9505-9ed0fb19d929",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_rf = tune_model(rf_model, choose_better=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07e13a4-9fc8-4360-a9ce-c5ccc8a8899f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(rf_model, plot = 'confusion_matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078f089d-ac2e-4b43-ab92-d9d4f4f19be5",
   "metadata": {},
   "source": [
    "### Predict on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a2cef6-febb-4c0a-adc7-eeb25145ef3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_model(tuned_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98824455-850c-4147-8c8f-07a60279e9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_model(tuned_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4080840b-e195-4725-9831-112552139591",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_model(tuned_ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f2f346-b5b2-45b9-a9a4-a7ae7269c937",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_model(rf_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c97bebe-62a1-482c-a5b5-b6f433e5a784",
   "metadata": {},
   "source": [
    "|           | Logistic Regression | SVM    | Ridge Classification | Random Forest |\n",
    "|-----------|---------------------|--------|----------------------|---------------|\n",
    "| Accuracy  | 0.7187              | 0.7148 | 0.7241               | 0.7074        |\n",
    "| Precision | 0.7155              | 0.7007 | 0.7194               | 0.7007        |\n",
    "| Recall    | 0.7261              | 0.7498 | 0.7350               | 0.7241        |\n",
    "| F1        | 0.7208              | 0.7244 | 0.7271               | 0.7122        |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d84ca41-82b8-4ea2-ae94-d32bb3aafccc",
   "metadata": {},
   "source": [
    "It is observed that the ridge classification actually gave the best accuracy score for the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70161d48-b0a6-4f74-a6e7-bcddeddbbef0",
   "metadata": {},
   "source": [
    "|       | Logistic Regression (1st model) | SVM    | Ridge Classification | Random Forest |\n",
    "|-------|---------------------------------|--------|----------------------|---------------|\n",
    "| Train | 0.842                           | 0.7109 | 0.7141               | 0.7059        |\n",
    "| Test  | 0.725                           | 0.7148 | 0.7241               | 0.7074        |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d72d61e-f00a-4a10-9562-52f32c5c065c",
   "metadata": {},
   "source": [
    "### Evaluating the models\n",
    "The final scores are depicted as such. It is observed that the 4 models produced similar result. All of them have an approximate accuracy score of 0.7 with the Logistic regression model edge ahead very slightly with it's score of 0.725. Which means 72.5% accuracy of correctly classifying the reddit post. The Pycaret logistic model did not beat the score of the original logistic model, although from the scores it is suggested that the model is overfitted. The other 3 models have very similar output scores on the test set, with the ridge classification being the best model out of the other 3.\n",
    "\n",
    "## Future work and recommendations\n",
    "This section is for future work to improve on the current model. The logistic model is great for binary classification and relatively inexpensive to compute results, however this current model is overfitted. For future work, these points can be considered\n",
    "- Use PDA to reduce the features\n",
    "- XGBoost to improve the model performance on random forest (although this is computationally expensive and will take longer computing time)\n",
    "- Collecting data on different seasons and put them through the same models for test because the logistic regression model might not be the best performer throughout the seasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0448ca-cb68-4b23-8c4d-889bbb3416aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_table = pd.DataFrame(list(X_train_tvec.columns)).copy()\n",
    "coef_table.insert(len(coef_table.columns),\"Coefs\",lr.coef_.transpose())\n",
    "coef_table.head()\n",
    "highest_10_coef = coef_table.sort_values(by=\"Coefs\",ascending = False).head(10)\n",
    "print(\"Top 10 words for starbucks\")\n",
    "highest_10_coef"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345f2b24-cad5-4bd4-b176-0f97ee8f9d34",
   "metadata": {},
   "source": [
    "Also this was observed, the top 10 words that are strong indicators that predict a post belongs to starbucks subreddit. Only frappuccino is the only food item on the list, which have to be crossed check with Dunkin Donuts menu and the word count of 'frappuccino' against the 5000 post scraped from reddit.\n",
    "\n",
    "- Sentiment Analysis should also be performed on the top words to determine if these keywords could potentially lead to an improvement of customer's experience.\n",
    "\n",
    "We embarked on this project to aim to solve achieve 2 objectives.\n",
    "- To discover if the new products are actually getting any buzz through reddit\n",
    "- To discover if it is of any value to Starbucks if reddit is a good source to validate our products in the future\n",
    "\n",
    "The new products indeed created buzz, by being among the top keywords appearing before preprocessing. It is also worth noting that reddit can be a platform to validate the response of starbucks new products in the future as the comments and sentiments derive from them are very raw and authentic.\n",
    "\n",
    "The top 10 words that strongly classify a post belonging to starbucks are\n",
    "- barista\n",
    "- partner\n",
    "- frappuccino\n",
    "- grand\n",
    "- art\n",
    "- transfer\n",
    "- shaken\n",
    "- flat\n",
    "- siren\n",
    "- store\n",
    "\n",
    "None of which includes our festive drinks (pumpkin spice)\n",
    "A good guess will be that the new products are no longer unique to starbucks and their competitors have already released similar products for this holiday.\n",
    "*In the future, new products can likewise be validated in the same way. Starbucks can validate future new products that are unique to their competitors by harvesting the data among the text in reddit and check if the list of new keywords contains any words that are related to the new products.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
